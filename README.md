# ğŸ“˜ Smart Study Agent  
### *A LangGraph + LangChain RAG Agent with Automatic OpenAI / Local LLM Switching + Optional Streamlit GUI*

---

## ğŸŒŸ Overview

**Smart Study Agent** is an intelligent, document-aware assistant built using:

- **LangChain** â†’ document loading, embeddings, retrieval  
- **LangGraph** â†’ agent workflow orchestration  
- **ChromaDB** â†’ vector database  
- **OpenAI or Ollama** â†’ LLM backend (auto-selected)  
- **HuggingFace embeddings** â†’ local fallback  
- **Streamlit GUI (optional)** â†’ clean chat interface  

Upload multiple PDFs â†’ the system ingests them â†’ then ask questions like:

- â€œSummarize Chapter 3â€
- â€œExplain this concept simplyâ€
- â€œGive me bullet-point notesâ€
- â€œCompare topic A vs topic Bâ€

This project showcases **LLM orchestration, RAG pipelines, embeddings, agents, vector search, and multimodal ingestion**â€”perfect for your AI/ML portfolio.

---

## ğŸš€ Features

### ğŸ” Retrieval-Augmented Generation (RAG)
Semantic search + contextual answers directly from user documents.

### ğŸ”„ Automatic Backend Switching

| Condition | LLM Backend | Embeddings |
|----------|-------------|------------|
| `.env` has API key | OpenAI GPT models | OpenAIEmbeddings |
| No API key / quota | Ollama (local) | HuggingFace MiniLM |

### ğŸ†“ 100% Free Local Mode
Works offline using:

- **Ollama + llama3**  
- **HuggingFace all-MiniLM-L6-v2 embeddings**

### ğŸ§© Modular Architecture
- Ingestion pipeline  
- RAG chain  
- LangGraph agent  
- Vector DB  
- Optional GUI  

### ğŸ–¥ï¸ Streamlit Web GUI (Optional)
A modern chat-style web interface to replace the terminal.

---

## ğŸ§© Architecture

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚       User Question      â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   LangGraph     â”‚
                     â”‚    Agent        â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚       RAG Pipeline       â”‚
                â”‚  (Runnable Composition)  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚          â”‚
                        â–¼          â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Retriever      â”‚   â”‚ Prompt Builder â”‚
            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ LLM (OpenAI/Ollama)  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚    Final Answer    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
smart_study_agent/
â”‚
â”œâ”€â”€ main.py                     # LangGraph terminal agent
â”œâ”€â”€ ui_app.py                   # Streamlit GUI
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ config.py               # OpenAI vs Local backend switch
â”‚   â”œâ”€â”€ ingest.py               # PDF â†’ Chroma ingestion
â”‚   â”œâ”€â”€ rag_chain.py            # RAG pipeline using Runnables
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ source/                 # User PDF uploads
â”‚   â””â”€â”€ chroma_db/              # Vector database
â”‚
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ install_dependencies.sh
â””â”€â”€ README.md
```

---

# ğŸ› ï¸ Installation

## 1. Clone the repository

```bash
git clone https://github.com/imYK223/smart_study_agent.git
cd smart_study_agent
```

## 2. Create virtual environment

```bash
python3 -m venv .venv
source .venv/bin/activate
```

## 3. Install dependencies

```bash
./install_dependencies.sh
```

Or manually:

```bash
pip install -r requirements.txt
```

## 4. (Optional) Install GUI dependencies

```bash
pip install streamlit
```

---

# ğŸ”Œ Backends: OpenAI or Local (Automatic)

## âœ… Option A â€” Use OpenAI (recommended for best quality)

1. Create a `.env` file (look at `.env.example`):

```env
OPENAI_API_KEY=sk-your-key-here
```

2. Run the application â€” it will automatically use:

- `ChatOpenAI`
- `OpenAIEmbeddings`

### ğŸ”‘ OpenAI Setup (Optional â€” for Best Model Quality)

If you want to use OpenAI models (GPT-4.1, GPT-4o, etc.) instead of the free local LLM, follow these steps:

#### â­ How to Get an OpenAI API Key
1. Visit: https://platform.openai.com/account/api-keys  
2. Log in with your OpenAI account  
3. Click **"Create new secret key"**  
4. Copy the key (it looks like: `sk-xxxxxx...`)

#### â­ Add Your Key to the Project
Create a `.env` file in the project root:

---

## ğŸ†“ Option B â€” Fully Local Mode (no API key)

If `.env` does *not* contain an API key, the system switches to:

- **LLM:** Ollama (`llama3` - size: 4.3GB, runs locally)  
- **Embeddings:** HuggingFace (`all-MiniLM-L6-v2`) 

### Install Ollama:

```bash
brew install ollama
ollama pull llama3
```

Ollama auto-runs in the background on macOS; you typically do **not** need to call `ollama serve` manually.

---

# ğŸ“¥ Step 1: Ingest Your Documents

Place PDFs or TXT files into:

```text
data/source/
```

Then run:

```bash
python -m app.ingest
```

This builds the Chroma vector DB.

---

# ğŸ¤– Step 2: 
## A. Run the Streamlit Web GUI (Recommended)

```bash
streamlit run ui_app.py
```

This launches a browser-based interface at:

```text
http://localhost:8501
```

You can upload PDFs, rebuild the index, and chat with the agent in a chat-like UI.

---
## B. Run the Smart Study Agent (Terminal Mode)

```bash
python main.py
```

This starts a simple CLI where you can type questions and get answers based on your documents.

---



## ğŸ–¥ï¸ Choosing Between GUI Mode and Terminal Mode

The project provides **two independent ways** to interact with the Smart Study Agent:

### 1ï¸âƒ£ Streamlit GUI (Recommended)
Run:

```bash
streamlit run ui_app.py
```

- No need to run `main.py` beforehand  
- Upload PDFs directly from the browser  
- Rebuild the index with a button  
- Chat-style interface with history  
- Best option for day-to-day usage and demos  

### 2ï¸âƒ£ Terminal Agent (CLI via LangGraph)

Run:

```bash
python main.py
```

- Uses the same RAG + LangGraph backend  
- Useful for debugging, quick tests, or when you prefer the terminal  
- No GUI required  

Both modes are **independent** â€” you can use either one at any time.

---

# ğŸ§ª Troubleshooting

### âŒ `Connection refused http://localhost:11434`
Ollama is not running.  
Fix:

```bash
open -a Ollama
```

---

### âŒ `listen tcp 127.0.0.1:11434: bind: address already in use`
Ollama is already running in the background â€” you do **not** need to run `ollama serve` manually.  Check with `curl http://localhost:11434/api/tags` or restart Ollama using `pkill -f Ollama && open -a Ollama`.

---

### âŒ â€œNo documents foundâ€
Add your files to:

```text
data/source/
```

Then rebuild the index:

```bash
python -m app.ingest
```

---

### âŒ OpenAI quota error (429)
You are out of OpenAI API credits.  
Remove `OPENAI_API_KEY` from `.env` â†’ the system automatically switches to **local mode** (Ollama + HuggingFace embeddings).

---

# ğŸ“ˆ Roadmap

- [x] Multi-PDF ingestion  
- [x] Automatic backend switching  
- [x] Local LLM support (llama3)  
- [x] Streamlit GUI  
- [ ] FastAPI REST backend  
- [ ] Web deployment (HuggingFace Spaces)  
- [ ] Multi-agent LangGraph workflow  
- [ ] Document citation extraction  

---

# ğŸ¤ Contributing

PRs and suggestions are welcome!

---

# â­ If this project helps you, please consider starring the repo!
