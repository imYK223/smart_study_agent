# ğŸ“˜ Smart Study Agent  
### *A LangGraph + LangChain RAG Agent with Automatic OpenAI / Local LLM Switching*

---

## ğŸŒŸ Overview

**Smart Study Agent** is an intelligent, document-aware assistant built using:

- **LangChain** â†’ document loading, embeddings, retrieval  
- **LangGraph** â†’ agent workflow  
- **ChromaDB** â†’ vector database  
- **OpenAI or Ollama** â†’ LLM backend  
- **HuggingFace embeddings** (fallback when no API key is provided)

Upload your PDFs or text notes â†’ the agent ingests them â†’ then ask it:

- â€œSummarize Chapter 3â€
- â€œExplain this in simple wordsâ€
- â€œGive bullet-point notesâ€
- â€œCompare Topic A and Topic Bâ€

This project demonstrates **RAG**, **agentic reasoning**, and **backend flexibility**, making it ideal for ML/AI portfolio use.

---

## ğŸš€ Features

### ğŸ” Retrieval-Augmented Generation (RAG)
The agent retrieves relevant text chunks from your documents and produces grounded answers.

### ğŸ”„ Automatic Backend Switching
No setup required:

| Condition | LLM Backend | Embeddings |
|----------|-------------|------------|
| **OPENAI_API_KEY is set** | OpenAI GPT Models | OpenAIEmbeddings |
| **No API key** | Local **Ollama** Model (llama3, phi3, etc.) | HuggingFace MiniLM |

### ğŸ†“ 100% Free Local Mode
If you donâ€™t set an OpenAI key, the system defaults to:

- **Ollama** (â€œllama3â€ by default)  
- **HuggingFace all-MiniLM-L6-v2 (local embeddings)**

### ğŸ§© Modular Architecture
- Ingestion pipeline  
- RAG chain (Runnable graph)  
- LangGraph agent node  
- Easily expandable  

### ğŸ§  Clean, modern LangChain v0.2+ API
Uses the latest Runnable & LangGraph patterns.

---

## ğŸ§© Architecture

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚       User Question      â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   LangGraph     â”‚
                     â”‚  (rag_node)     â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚       RAG Pipeline       â”‚
                â”‚  (Runnable composition)  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚          â”‚
                        â–¼          â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Retriever      â”‚   â”‚ Prompt Builder â”‚
            â”‚ (ChromaDB)     â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ LLM (OpenAI/Ollama)  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Final Answer     â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
smart_study_agent/
â”‚
â”œâ”€â”€ main.py                   # LangGraph agent
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ config.py             # OpenAI vs Local (Ollama) backend switch
â”‚   â”œâ”€â”€ ingest.py             # PDF/Text â†’ ChromaDB vectorstore
â”‚   â”œâ”€â”€ rag_chain.py          # RAG pipeline using Runnables
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ source/               # Put your PDFs here
â”‚   â””â”€â”€ chroma_db/            # Auto-generated vector DB
â”‚
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ install_dependencies.sh
â””â”€â”€ README.md
```

---

# ğŸ› ï¸ Installation

## 1. Clone the repository

```bash
git clone https://github.com/your-username/smart_study_agent
cd smart_study_agent
```

## 2. Create virtual environment

```bash
python3 -m venv .venv
source .venv/bin/activate
```

## 3. Install dependencies

```bash
./install_dependencies.sh
```

Or manually:

```bash
pip install -r requirements.txt
```

---

# ğŸ”Œ Backends: OpenAI or Local (Automatic)

## âœ… Option A â€” Use OpenAI (recommended for best quality)

1. Create a `.env` file (look at `.env.example`):

```env
OPENAI_API_KEY=sk-your-key-here
```

2. Run the application â€” it will automatically use:

- `ChatOpenAI`
- `OpenAIEmbeddings`

### ğŸ”‘ OpenAI Setup (Optional â€” for Best Model Quality)

If you want to use OpenAI models (GPT-4.1, GPT-4o, etc.) instead of the free local LLM, follow these steps:

#### â­ How to Get an OpenAI API Key
1. Visit: https://platform.openai.com/account/api-keys  
2. Log in with your OpenAI account  
3. Click **"Create new secret key"**  
4. Copy the key (it looks like: `sk-xxxxxx...`)

#### â­ Add Your Key to the Project
Create a `.env` file in the project root:

---

## ğŸ†“ Option B â€” Fully Local Mode (no API key)

If `.env` does *not* contain an API key, the system switches to:

- **LLM:** Ollama (`llama3` - size: 4.3GB, runs locally)  
- **Embeddings:** HuggingFace (`all-MiniLM-L6-v2`)  

### Install & Run Ollama

1. Download Ollama: https://ollama.com  
2. Pull a model:

```bash
ollama pull llama3
```

3. Start server:

```bash
ollama serve
```

---

# ğŸ“¥ Step 1: Ingest Your Documents

Place PDFs or text files in:

```
data/source/
```

Then run:

```bash
python -m app.ingest
```

This will:

- Load PDFs  
- Split text into chunks  
- Create embeddings  
- Save them inside `data/chroma_db/`  

---

# ğŸ¤– Step 2: Run the Smart Study Agent

```bash
python main.py
```

Example interaction:

```
Question: summarize chapter 3
Answer: ...
```

Examples you can try:

- summarize chapter 3  
- explain rigid body transformations  
- list 5 key points from chapter 2  
- compare foundation pose and classical methods  

---

# ğŸ§ª Troubleshooting

### âŒ Error: `Connection refused http://localhost:11434`
You are in **local mode**, but Ollama is not running.

Fix:

```bash
ollama serve
```

---

### âŒ Error: `Listen tcp 127.0.0.1:11434: bind: address already in use` 
This means Ollama is already running in the background â€” you do NOT need to run `ollama serve` manually.  
Check with `curl http://localhost:11434/api/tags` or restart Ollama using `pkill -f Ollama && open -a Ollama`.
 
---

### âŒ â€œNo documents foundâ€
Ensure your files are inside:

```
data/source/
```

---

### âŒ Irrelevant RAG output  
Try adjusting:

- Chunk size  
- Chunk overlap  
- More source documents  

---

# ğŸ“ˆ Roadmap (Future Enhancements)

- [ ] REST API (FastAPI)  
- [ ] Web UI (Streamlit / React)  
- [ ] Multi-agent LangGraph workflow  
- [ ] PDF citation extraction  
- [ ] Online document upload  
- [ ] Reranking & semantic filtering  

---

# ğŸ¤ Contributing
PRs and suggestions are welcome!

---

# â­ If you find this project useful, please consider starring the repo!
